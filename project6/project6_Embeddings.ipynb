{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# üß† **Understanding Embeddings with BERT**\n",
        "\n",
        "In this notebook, we go into the world of embeddings, utilizing the BERT model to understand the semantic relationships between words in different contexts.\n",
        "\n",
        "## üõ†Ô∏è Setup and Installation\n",
        "\n",
        "Start by installing the necessary libraries to ensure all functionalities are available."
      ],
      "metadata": {
        "id": "yUuby3g8ouzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.29.2\n",
        "!pip install scipy==1.7.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6C1auKaP5I93",
        "outputId": "495921be-4b51-4182-af3b-295d6cd7683e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.29.2\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.29.2)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.29.2) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.29.2) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.2) (2024.2.2)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.2\n",
            "    Uninstalling tokenizers-0.15.2:\n",
            "      Successfully uninstalled tokenizers-0.15.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.38.2\n",
            "    Uninstalling transformers-4.38.2:\n",
            "      Successfully uninstalled transformers-4.38.2\n",
            "Successfully installed tokenizers-0.13.3 transformers-4.29.2\n",
            "Collecting scipy==1.7.3\n",
            "  Downloading scipy-1.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.9 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy<1.23.0,>=1.16.5 (from scipy==1.7.3)\n",
            "  Downloading numpy-1.22.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.11.4\n",
            "    Uninstalling scipy-1.11.4:\n",
            "      Successfully uninstalled scipy-1.11.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "arviz 0.15.1 requires scipy>=1.8.0, but you have scipy 1.7.3 which is incompatible.\n",
            "chex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.22.4 which is incompatible.\n",
            "jax 0.4.26 requires scipy>=1.9, but you have scipy 1.7.3 which is incompatible.\n",
            "jaxlib 0.4.26+cuda12.cudnn89 requires scipy>=1.9, but you have scipy 1.7.3 which is incompatible.\n",
            "pandas-stubs 2.0.3.230814 requires numpy>=1.25.0; python_version >= \"3.9\", but you have numpy 1.22.4 which is incompatible.\n",
            "plotnine 0.12.4 requires numpy>=1.23.0, but you have numpy 1.22.4 which is incompatible.\n",
            "tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 1.22.4 which is incompatible.\n",
            "xarray-einstats 0.7.0 requires scipy>=1.8, but you have scipy 1.7.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.22.4 scipy-1.7.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "989715c3d2b24216a2e70bb2f885cef2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìö Importing Libraries\n",
        "\n",
        "Import essential modules for our tasks."
      ],
      "metadata": {
        "id": "mpAjguA9o7tT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel, AutoTokenizer\n",
        "from scipy.spatial.distance import cosine"
      ],
      "metadata": {
        "id": "DBwmRlXP5zY6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ü§ñ Model Setup\n",
        "\n",
        "Load the pre-trained BERT model and tokenizer. This model will help us extract embeddings for our analysis."
      ],
      "metadata": {
        "id": "XsfQ55vto_Bd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the model name\n",
        "model_name = \"bert-base-cased\"\n",
        "\n",
        "# Loading the pre-trained model and tokenizer\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-JQ5YWeJB6Y",
        "outputId": "2339c7a7-9f04-490d-ac0a-d691668c4544"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## üìù Function Definition: Predict\n",
        "\n",
        "Define a function that encodes input text into tensors, which are then fed to the model to obtain embeddings."
      ],
      "metadata": {
        "id": "ZzoQD4KrpClk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a function to encode the input text and get model predictions\n",
        "def predict(text):\n",
        "    encoded_inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    return model(**encoded_inputs)[0]"
      ],
      "metadata": {
        "id": "GJIJ1VUZJBtp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìÉ Defining the Sentences\n",
        "\n",
        "Set up sentences to analyze. The"
      ],
      "metadata": {
        "id": "eGyuVbYkpEfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the sentences\n",
        "sentence1 = \"There was a fly drinking from my soup\"\n",
        "sentence2 = \"There is a fly swimming in my juice\"\n",
        "# sentence2 = \"To become a commercial pilot, he had to fly for 1500 hours.\" # second fly example\n",
        "\n",
        "# Tokenizing the sentences\n",
        "tokens1 = tokenizer.tokenize(sentence1)\n",
        "tokens2 = tokenizer.tokenize(sentence2)"
      ],
      "metadata": {
        "id": "s_0dKdRbJFtL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîç Tokenization and Model Predictions\n",
        "\n",
        "Tokenize the sentences and obtain predictions (embeddings) from the model."
      ],
      "metadata": {
        "id": "fTn6NoNepG16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting model predictions for the sentences\n",
        "out1 = predict(sentence1)\n",
        "out2 = predict(sentence2)"
      ],
      "metadata": {
        "id": "5B0LO9xfJI2P"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîÑ Extracting Embeddings\n",
        "\n",
        "Extract embeddings specifically for the word \"fly\" from both sentences."
      ],
      "metadata": {
        "id": "Mb8tPoc3pJxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting embeddings for the word 'fly' in both sentences\n",
        "emb1 = out1[0:, tokens1.index(\"fly\"), :].detach()[0]\n",
        "emb2 = out2[0:, tokens2.index(\"fly\"), :].detach()[0]\n",
        "\n",
        "# emb1 = out1[0:, 3, :].detach()\n",
        "# emb2 = out2[0:, 3, :].detach()"
      ],
      "metadata": {
        "id": "y8ZvAkGiJLZf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Calculating Cosine Similarity\n",
        "\n",
        "Calculate the cosine similarity between the embeddings of the word \"fly\" from both sentences to measure how context affects meaning."
      ],
      "metadata": {
        "id": "gcvcIGiOpLyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the cosine similarity between the embeddings\n",
        "cosine(emb1, emb2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKVSZUxyJNZs",
        "outputId": "b389c0d3-3650-45a2-9b4a-8b425f8a57ef"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.06798791885375977"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üåü Conclusion\n",
        "\n",
        "This notebook has guided you through the process of extracting and comparing word embeddings using BERT. Such techniques are fundamental in understanding word semantics and their usage across different contexts.\n",
        "\n",
        "Experiment by changing the sentences or focusing on different words to see how the embeddings and their similarities vary!"
      ],
      "metadata": {
        "id": "gfFR3mbspN8y"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  }
}